PS C:\> .\.venv\Scripts\activate.ps1
(.venv) PS C:\> python .\llama_cpp_fetcher.py
Starting the download process...
Fetching the latest release information from GitHub...
Latest release information fetched successfully.
Detecting system information...
System: windows, Architecture: amd64
Checking CUDA version using nvidia-smi...
Detected CUDA version: 12.5
Checking driver version using nvidia-smi...
Detected driver version: 555.85
NVIDIA GPU detected: True, CUDA version: 12.5, Driver version: 555.85
Checking CPU for AVX support...
AVX: True, AVX2: True, AVX512: False
Selecting the best asset for the system...
Extracting available CUDA versions from assets...
Available CUDA versions: ['12.2.0', '11.7.1']
Checking CUDA version 12.2.0 which requires driver version 536.25
Driver version 555.85 is sufficient for CUDA version 12.2.0
Selected asset: llama-b3089-bin-win-cuda-cu12.2.0-x64.zip for CUDA version 12.2.0
Downloading asset from https://github.com/ggerganov/llama.cpp/releases/download/b3089/llama-b3089-bin-win-cuda-cu12.2.0-x64.zip...
Downloaded asset to downloads\llama-b3089-bin-win-cuda-cu12.2.0-x64.zip
Extracting zip file...
Extraction complete.
Download process completed.
Running main.exe with '--version' to verify...
version: 3089 (c90dbe02)
built with MSVC 19.39.33523.0 for x64

llama_cpp_fetcher.py: It really whips the llama's ass!
