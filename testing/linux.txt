> podman build -t fetch_llama_cpp .
> podman run -it fetch_llama_cpp
Starting the download process...
Fetching the latest release information from GitHub...
Latest release information fetched successfully.
Detecting system information...
System: linux, Architecture: x86_64
Checking CUDA version using nvidia-smi...
nvidia-smi not found. No NVIDIA GPU detected.
Checking driver version using nvidia-smi...
nvidia-smi not found. No NVIDIA GPU detected.
NVIDIA GPU detected: False, CUDA version: None, Driver version: None
Checking for AMD GPU using lspci...
lspci not found. No AMD GPU detected.
Checking CPU for AVX support...
AVX: True, AVX2: True, AVX512: False
Selecting the best asset for the system...
Extracting available CUDA versions from assets...
Available CUDA versions: ['12.2.0', '11.7.1']
Checking CUDA version 12.2.0 which requires driver version 535.54.03
Driver version None is not sufficient or not detected for CUDA version 12.2.0
Checking CUDA version 11.7.1 which requires driver version 515.48.07
Driver version None is not sufficient or not detected for CUDA version 11.7.1
No compatible CUDA version found. Falling back to CPU-only option.
Selected CPU-only asset: llama-b3089-bin-ubuntu-x64.zip
Downloading asset from https://github.com/ggerganov/llama.cpp/releases/download/b3089/llama-b3089-bin-ubuntu-x64.zip...
Downloaded asset to downloads/llama-b3089-bin-ubuntu-x64.zip
Extracting zip file...
Set execute permissions for binaries on POSIX system.
Extraction complete.
Download process completed.
Running main with '--version' to verify...
version: 3089 (c90dbe02)
built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu

fetch_llama_cpp.py: It really whips the llama's ass!
